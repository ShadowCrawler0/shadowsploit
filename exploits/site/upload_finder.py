import requests
from bs4 import BeautifulSoup
import argparse
from urllib.parse import urljoin
import re

def find_upload_forms(url):
    try:
        resp = requests.get(url, timeout=10)
    except Exception as e:
        print(f"[-] Failed to fetch {url}: {e}")
        return

    soup = BeautifulSoup(resp.text, 'html.parser')
    forms = soup.find_all('form')

    found = False
    for i, form in enumerate(forms):
        file_inputs = form.find_all('input', {'type': 'file'})
        if file_inputs:
            found = True
            print(f"\n[+] Possible upload form found on {url}")
            print(f"Form #{i + 1}:")
            print(f" - Method: {form.get('method', 'GET').upper()}")
            print(f" - Action: {form.get('action')}")
            print(f" - Enctype: {form.get('enctype')}")
            print(f" - File input fields: {[input.get('name') for input in file_inputs]}")

def crawl_and_scan(base_url, depth=2, visited=None):
    if visited is None:
        visited = set()
    if depth == 0 or base_url in visited:
        return

    visited.add(base_url)
    try:
        resp = requests.get(base_url, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        find_upload_forms(base_url)

        # Extract internal links to crawl
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('http'):
                next_url = href
            elif href.startswith('/'):
                next_url = urljoin(base_url, href)
            else:
                continue

            if base_url.split('/')[2] in next_url:
                crawl_and_scan(next_url, depth - 1, visited)

    except Exception as e:
        print(f"[-] Error crawling {base_url}: {e}")

def main():
    parser = argparse.ArgumentParser(description='File Upload Form Finder')
    parser.add_argument('url', help='Target base URL (e.g., http://example.com)')
    parser.add_argument('--depth', type=int, default=2, help='Crawl depth (default: 2)')
    args = parser.parse_args()

    print(f"[*] Scanning {args.url} for upload forms (depth={args.depth})...")
    crawl_and_scan(args.url, depth=args.depth)

if __name__ == "__main__":
    main()
