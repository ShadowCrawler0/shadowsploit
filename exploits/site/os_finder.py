import requests
import argparse

def analyze_headers(headers):
    os_guesses = []

    server = headers.get('Server', '').lower()
    x_powered_by = headers.get('X-Powered-By', '').lower()
    via = headers.get('Via', '').lower()
    # Add other headers to analyze as needed

    # Simple keyword matching for OS
    if 'windows' in server or 'windows' in x_powered_by:
        os_guesses.append('Windows Server')
    if 'linux' in server or 'linux' in x_powered_by:
        os_guesses.append('Linux')
    if 'unix' in server or 'unix' in x_powered_by:
        os_guesses.append('Unix')
    if 'apache' in server:
        os_guesses.append('Likely Linux/Unix (Apache)')
    if 'iis' in server:
        os_guesses.append('Likely Windows (IIS)')
    if 'nginx' in server:
        os_guesses.append('Likely Linux/Unix (nginx)')
    if 'ubuntu' in server or 'ubuntu' in x_powered_by:
        os_guesses.append('Linux (Ubuntu)')
    if 'centos' in server or 'centos' in x_powered_by:
        os_guesses.append('Linux (CentOS)')

    return os_guesses

def fetch_robots_txt(url):
    try:
        robots_url = url.rstrip('/') + '/robots.txt'
        resp = requests.get(robots_url, timeout=10)
        if resp.status_code == 200:
            return resp.text
    except:
        pass
    return None

def detect_os_by_error_page(url):
    # A basic heuristic could be fetching a non-existent page to trigger error pages
    try:
        test_url = url.rstrip('/') + '/nonexistentpage12345.html'
        resp = requests.get(test_url, timeout=10)
        content = resp.text.lower()
        if 'microsoft' in content or 'iis' in content:
            return 'Windows Server (IIS)'
        if 'apache' in content:
            return 'Linux/Unix (Apache)'
        if 'nginx' in content:
            return 'Linux/Unix (nginx)'
    except:
        pass
    return None

def main():
    parser = argparse.ArgumentParser(description='Basic OS Fingerprinting for Web Servers')
    parser.add_argument('url', help='Target website URL (e.g. http://example.com)')
    args = parser.parse_args()

    try:
        resp = requests.get(args.url, timeout=10)
        print(f"[+] HTTP Status: {resp.status_code}")
        print(f"[+] Server Headers: {resp.headers.get('Server', 'N/A')}")
        os_guesses = analyze_headers(resp.headers)

        if os_guesses:
            print("[*] Possible Operating Systems based on headers:")
            for guess in set(os_guesses):
                print(f" - {guess}")
        else:
            print("[-] No clear OS info from headers.")

        # Try error page detection
        err_os = detect_os_by_error_page(args.url)
        if err_os:
            print(f"[*] OS guess from error page: {err_os}")

        # Optional: analyze robots.txt for any hints
        robots_txt = fetch_robots_txt(args.url)
        if robots_txt:
            print("[*] robots.txt found, analyzing...")
            if 'windows' in robots_txt.lower():
                print(" - Possible Windows server based on robots.txt content.")
            elif 'linux' in robots_txt.lower():
                print(" - Possible Linux server based on robots.txt content.")
    except Exception as e:
        print(f"[-] Failed to fetch or analyze target: {e}")

if __name__ == "__main__":
    main()
