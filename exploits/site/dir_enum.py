import requests
import argparse
import time
from urllib.parse import urljoin

def check_paths(base_url, wordlist_file, delay):
    with open(wordlist_file, 'r') as f:
        paths = [line.strip() for line in f if line.strip()]

    print(f"Starting scan on {base_url} with {len(paths)} paths.\n")
    for path in paths:
        url = urljoin(base_url, path)
        try:
            response = requests.get(url, allow_redirects=True, timeout=10)
            print(f"{url} -> {response.status_code}")
        except requests.RequestException as e:
            print(f"{url} -> Error: {e}")

        time.sleep(delay)

def main():
    parser = argparse.ArgumentParser(description="Simple Directory/File Enumerator")
    parser.add_argument('url', help='Base URL to scan (e.g., https://example.com/)')
    parser.add_argument('wordlist', help='Path to wordlist file')
    parser.add_argument('--delay', type=float, default=0.5, help='Delay between requests in seconds (default 0.5)')

    args = parser.parse_args()
    base_url = args.url
    if not base_url.endswith('/'):
        base_url += '/'

    check_paths(base_url, args.wordlist, args.delay)

if __name__ == "__main__":
    main()
